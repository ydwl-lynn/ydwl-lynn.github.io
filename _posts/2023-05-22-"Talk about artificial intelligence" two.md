---
layout: post
title: 《浅谈人工智能》——摘录·下
category: 摘录
tag: 博客
---

- *
{:toc}

## 第五节 计算机视觉：从“深”到“暗” Dark,Beyond Deep

视觉是人脑最主要的信息来源，也是进入人工智能这个殿堂的大门。图像是一个像素的二维矩阵，可是我们感知到非常丰富的三维场景、行为的信息；你看的时间越长，理解的也越多。下面列举几个被主流（指大多数研究人员）忽视的，但是很关键的研究问题。

一、几何常识推理与三维场景构建。

![figure](/images/2023/0521/figure.png){:width="550px"}

以前计算机视觉的研究，需要通过多张图像（多视角）之间特征点的对应关系，去计算这些点在三维世界坐标系的位置（SfM、SLAM）。其实人只需要一张图像就可以把三维几何估算出来。因为在我们的人造环境中，有很多几何常识和规律：比如你坐的椅子高度就是你小腿的长度约 16 英寸，桌子约 30 英寸，案台约 35 英寸，门高约 80 英寸 --- 都是按照人的身体尺寸和动作来设计的。另外，人造环境中有很多重复的东西，比如几个窗户一样，大小一致，建筑设计和城市规划都有规则。这些都是geometric common sense。根据这些几何的约束就可以定位很多点的三维位置，同时估计相机位置和光轴。

下面这张图拍摄的是我家厨房，在这个三维场景中，我们的理解就可以表达成为一个层次分解（compositional）的时空因果的解译图（Spatial，Temporal and Causal Parse 
Graph）,简称 STC-PG。

![fig4](/images/2023/0521/fig4.png){:width="550px"}

几何重建的一个很重要的背景是，我们往往不需要追求十分精确的深度位置。比如，人对三维的感知其实都是非常不准的，它的精确度取决于你当前要执行的任务。在执行的过程中，你不断地根据需要来提高精度。比如，你要去拿几米以外的一个杯子，一开始你对杯子的方位只是一个大致的估计，在你走近、伸手的过程中逐步调整精度。这就回到上一节谈的问题，不同任务对几何与识别的精度要求不一样。这是人脑计算非常高效的一个重要原因。

二、场景识别的本质是功能推理。

现在很多学者做场景的分类和分割都是用一些图像特征，用大量的图片例子和手工标注的结果去训练神经网络模型 --- 这是典型的“鹦鹉”模式。而一个场景的定义本质上就是功能。当你看到一个三维空间之后，人脑很快就可以想象我可以干什么：这个地方倒水，这里可以拿杯子，这里可以坐着看电视等。现代的设计往往是复合的空间，就是一个房间可以多种功能，所
以简单去分类已经不合适了。比如，美式厨房可以做饭、洗菜、用餐、聊天、吃饭。卧室可以睡觉、梳妆、放衣服、看书。场景的定义是按照你在里面能够干什么，这个场景就是个什么，按照功能划分，这些动作都是你想象出来的，实际图像中并没有。人脑感知的识别区与运动规划区是直接互通的，相互影响。

为了想象这些功能，人脑有十分丰富的动作模型，这些动作根据尺度分为两类（见下图）。第一类（左图）是与整个身体相关的动作，如坐、站、睡觉、工作等等；第二类（右图）是与手的动作相关的，如砸、剁、锯、撬等等。这些四维基本模型（三维空间加一维时间）可以通过日常活动记录下来，表达了人的动作和家具之间，以及手和工具之间的关系。正因为这一点，心理学研究发现我们将物体分成两大类，分别存放在脑皮层不同区域：一类是跟手的大小有关，跟手的动作相关的，如你桌上的东西；另一类是跟身体有关，例如家具之类。

![fig5](/images/2023/0521/fig5.png){:width="550px"}

有了这个理解，我们就知道：下面两张图，虽然图像特征完全不同，但是他们是同一类场景,功能上是等价的。人的活动和行为，不管你是哪个国家、哪个历史时期，基本是不变的。这是“智能泛化”的基础，也就是把你放到一个新的地区，你不需要大数据训练，马上就能理解、适应。这是我们能够举一反三的一个基础。

![fig6](/images/2023/0521/fig6.png){:width="550px"}

回到前面的那个 STC-PG 解译图，每个场景底下其实就分解成为一些动作和功能 （见 STC-PG 图中的绿色方片节点）。由计算机想象、推理的各种功能决定对场景的分类。 想象功能就是把人的各种姿态放到三维场景中去拟合（见厨房解译图中人体线画）。这是完全不同于当前的深度学习方法用的分类方法。

三、物理稳定性与关系的推理。

回到前面的那个 STC-PG 解译图，每个场景底下其实就分解成为一些动作和功能 （见 STC-PG 图中的绿色方片节点）。由计算机想象、推理的各种功能决定对场景的分类。 想象功能就是把人的各种姿态放到三维场景中去拟合（见厨房解译图中人体线画）。这是完全不同于当前的深度学习方法用的分类方法。

我们对图像的理解包含了物体之间的物理关系，每个物体的支撑点在那里。比如，下面这个图，吊灯和墙上挂的东西，如果没有支撑点，就会掉下来（右图）。

![fig7](/images/2023/0521/fig7.png){:width="550px"}

我提出了一个新的场景理解的 minimax 标准：minimize instability and maximize functionality“最小化不稳定性且最大化功能性”。这比以前我们做图像理解的用的 MDL（最小描述长度）标准要更靠谱。这是解决计算机视觉的基本原理，功能和物理是设计场景的基本原则。几何尺寸是附属于功能推出来的，比如椅子的高度就是因为你要坐得舒服，所以就是你小腿的长度。

回到我家厨房的例子，你就会问，那里面的水是如何被检测到的呢？水是看不见的，花瓶和水壶里的水由各种方式推出来的。另外，你可能注意到，桌上的番茄酱瓶子是倒立着，为什么呢？ 你可能很清楚，你家的洗头膏快用完的时候，瓶子是不是也是的倒着放的呢？这就是对粘稠液体的物理和功能理解之后的结果。由此，你可以看到我们对一个场景的理解是何等“深刻”，远远超过了用深度学习来做的物体分类和检测。

四、意向、注意和预测。

厨房那张图有一个人和一只狗，我们可以进一步识别其动作、眼睛注视的地方，由此推导其动机和意向。这样我们可以计算她在干什么、想干什么，比如说她现在是渴了，还是累了。通过时间累积之后，进而知道她知道哪些，也就是她看到了或者没有看到什么。在时间上做预测，她下面想干什么。只有把这些都计算出来了，机器才能更好地与人进行交互。

所以，虽然我们只看到一张图片，那张 STC-PG 中，我们增加了时间维度，对人和动物的之前和之后的动作，做一个层次的分析和预测。当机器人能够预判别人的意图和下面的动作，那么它才能和人进行互动和合作。后面，我们讲的语言对话可以帮助人机互动和合作；但是，我们日常很多交互协助，靠的是默契，不需要言语也能做不少事。

五、任务驱动的因果推理与学习。

前面我谈了场景的理解的例子，下面我谈一下物体的识别和理解，以及为什么我们不需要大数据的学习模式，而是靠举一反三的能力。

我们的知识是根据我们的任务来组织的。那么什么叫做任务呢？如何表达成数学描述呢？

每个任务其实是在改变场景中的某些物体的状态。牛顿发明了一个词，在这里被借用了：叫做 fluent。这个词还没被翻译到中文，就是一种可以改变的状态，我暂且翻译为“流态”吧。比如，把水烧开，水温就是一个流态；番茄酱与瓶子的空间位置关系是一个流态，可以被挤出来；还有一些流态是人的生物状态，比如饿、累、喜悦、悲痛；或者社会关系：从一般人，到朋友、再到密友等。人类和动物忙忙碌碌，都是在改变各种流态，以提高我们的价值函数（利益）。

懂得这一点，我们再来谈理解图像中的三维场景和人的动作。其实，这就是因果关系的推理。所谓因果就是：人的动作导致了某种流态的改变。理解图像其实与侦探(福尔摩斯)破案一样，他需要的数据往往就是很小的蛛丝马迹，但是，他能看到这些蛛丝马迹，而普通没有受侦探训练的人就看不见。那么，如何才能看到这些蛛丝马迹呢？

其一、你需要大量的知识，这个知识来源于图像之外，是你想象的过程中用到的，比如一个头发怎么掉在这里的？（如今的大规模预训练模型是否可以表示一个拥有大量知识的模型？）
其二，行为的动机目的，犯案人员到底想改变什么“流态”？

我把这些图像之外的东西统称为“暗物质”--- Dark Matter。物理学家认为我们可观察的物质和能量只是占宇宙总体的 5%，剩下的 95%是观察不到的暗物质和暗能量。视觉与此十分相似：感知的图像往往只占 5%，提供一些蛛丝马迹；而后面的 95%，包括功能、物理、因果、动机等等是要靠人的想象和推理过程来完成的。

举例说明，如下图所示，一个人要完成的任务是砸核桃，改变桌子上那个核桃的流态。把这个任务交给一个学生，他从桌面上的工具里面选择了一个锤子，整个过程没有任何过人之处，因为你也会这么做。

![fig8](/images/2023/0521/fig8.png){:width="550px"}

不过你细想一下，这个问题还相当复杂。这个动作就包含了很多信息：他为什么选这个锤子而不选别的东西，他为什么拿着锤这个柄靠后的位置？他挥动的力度用多少，这都是经过计算的。这还有几千几万的可能其他各种选择、解法，他没有选择，说明他这个选法比其它的选择肯定会好，好在哪呢？看似简单的问题，往往很关键，一般人往往忽略了。

现在到一个新的场景（见上图右），原来学习的那些工具都不存在了，完全是新的场景和物体，任务保持不变。你再来砸这个核桃试试看，怎么办？人当然没有问题，选这个木头做的桌子腿，然后砸的动作也不一样。这才是举一反三，这才是智能，这没有什么其他数据，没有大量数据训练，这不是深度学习方法。

那这个算法怎么做的呢？我们把对这个物理空间、动作、因果的理解还是表达成为一个Spatial，Temporal and Causal Parse Graph（STC-PG）。这个 STC-PG 包含了你对空间的理解（物体、三维形状、材质等）、时间上动作的规划、因果的推理。最好是这样子砸，它物理因果能够实现，可能会被砸开，再连在一块来求解，求时间、空间和因果的这么一个解析图，就是一个解。也就是，最后你达到目的，改变了某种物理的流态。

我再强调几点：

一、这个 STC-PG 的表达是你想象出来的。这个理解的过程是在你动手之前就想好了的，它里面的节点和边界大多数在图像中是没有的，也就是我称作的“暗物质”。

二、这个计算的过程中，大量的运算属于“top-down”自顶向下的计算过程。也就是用你脑皮层里面学习到的大量的知识来解释你看到的“蛛丝马迹”，形成一个合理的解。而这种 Top-down 的计算过程在目前的深度多层神经网络中是没有的。

三、学习这个任务只需要极少的几个例子。如果一个人要太多的例子，说明Ta脑袋“不开窍”，智商不够。子曰：“学而不思则罔，思而不学则殆”。这里的“思”应该是推理，对于自然界或者社会的现象、行为和任务，形成一个符合规律的自洽的解释，在我看来就是一个STC-PG。

那么 STC-PG 是如何推导出来的呢？它的母板是一个 STC-AOG，AOG 就是 And-Or Graph 与或图。这个与或图是一个复杂的概率语法图模型，它可以导出巨量的合乎规则的概率事件，每一个事件就是 STC-PG。这个表达与语言、认知、机器人等领域是一致的。在我看来，这个STC-AOG 是一个统一表达，它与逻辑以及 DNN 可以打通关节。这里就不多讲了。

计算机视觉小结：

视觉研究前面25年的主流是做几何，以形状和物体为中心的研究：Geometry-Based and Object-Centered。最近25年是从图像视角通过提取丰富的图像特征描述物体的外观来做识别、分类：Appearance-Based and View-Centered。几何当然决定表观。那么几何后面深处原因是什么呢？几何形状的设计是因为有任务，最顶层是有任务，然后考虑到功能、物理、因果，设计了这些物体再来产生图像，这是核心问题所在。我把在当前图像是看不见的“东西”叫 dark matter。物理里面 dark matter energy 占 95%，确确实实在我们智能里面 dark matter 也占了大部分。而你看到的东西就是现在深度学习能够解决的，比如说人脸识别、语音识别，就是很小的一部分看得见的东西；看不见的在后面，才是我们真正的智能，像那个
乌鸦能做到的。

所以，我的一个理念是：计算机视觉要继续发展，必须发掘这些“dark matter”。把图像中想象的 95%的暗物质与图像中可见的 5%的蛛丝马迹，结合起来思考，才能到达真正的理解。现在大家都喜欢在自己工作前面加一个 Deep，以为这样就算深刻了、深沉了，但其实还是非常肤浅的。不管你多深，不管你卷积神经网络多少层，它只是处理可见的图像表观特征、语音特征，没有跳出那 5%，对吧？那些认为深度学习解决了计算机视觉的同学，我说服你了么？如果没有，后面还有更多的内容。

视觉研究的未来，我用一句话来说：Go Dark， Beyond Deep --- 发掘暗，超越深。

这样一来，视觉就跟认知和语言接轨了。